---
title: 'Stat542: Hidden Markov Model'
output:
  html_notebook:
    toc: yes
    theme: readable
---

```{r global_options,echo=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.height=6)
options(digits = 4)
```

### The Dishonest Casino

This example is taken from Durbin et. al. (1999): A dishonest casino uses two dice, one of them is fair the other is loaded. 

* The probabilities of the fair die are (1/6,...,1/6). 
* The probabilities of the loaded die are (1/10,...,1/10,1/2)

The observer doesn't know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.

```{r fig.width=6}
library(HMM)
dishonestCasino()
```

### A Simulated Example

Specify the following HMM. 

```{r}
A = matrix(c(0.95,0.05,0.05,0.95),2,2);
B = matrix(c(1/3, 0.1, 1/3, 0.2, 1/3,0.7),2,3);
hmm = initHMM(c("A", "B"), c(1, 2, 3),
            startProbs=c(1/2, 1/2),
            transProbs=A, emissionProbs=B)
print(hmm)
```

Generate n=500 obs from this HMM. 

```{r}
set.seed(100)
n = 500; 
data = simHMM(hmm, n)
names(data)
cbind(data$states[1:5], data$observation[1:5])
```

### Fit the data with two hidden states

#### Use BaumWelch (i.e., EM) algorithm to estimate the model parameters. 

Note that the algorithm would get stuck, if we initialize the emission distributions to be the same for each latent state.

```{r}
tmphmm = initHMM(c("A", "B"), c(1, 2, 3))
print(tmphmm)
myfit2 = baumWelch(tmphmm, data$obs)
names(myfit2)
print(myfit2$hmm)
```

Next we initilize the emission probability matrix B as follows: generate positive entries for the 2-by-3 matrix using a gamma distribution, and then divide each entry by the corresponding row sum to ensure each row is a probability vector summing to 1. 

```{r}
set.seed(100)
tmpB = matrix(rgamma(n=6, shape=1), 2, 3)
tmpB = tmpB / rowSums(tmpB)
tmphmm = initHMM(c("A", "B"), c(1, 2, 3),
               emissionProbs = tmpB)
print(tmphmm)
myfit2 = baumWelch(tmphmm, data$obs)
names(myfit2)
print(myfit2$hmm)
```


#### Predict Latent States using Individual Marginal Probabilities

Compute the (marginal) conditional distribution of the hidden state for each observation: P(Z_i = k | X_1, ..., X_n). Then classify Z_i to be the most probable state. 

```{r}
mypost = posterior(myfit2$hmm, data$obs)
```

#### Predict Latent States using the Viterbi algorithm

Use the Viterbi algorithm to compute the most probable hidden sequence.

```{r}
vit.out = viterbi(myfit2$hmm, data$obs)
```

#### Display the result

```{r fig.width=6}
# plot the data
plot(data$obs, ylim = c(-6, 3), pch = 3, 
     xlab = "", ylab = "", bty = "n", yaxt = "n")
axis(2, at = 1:3)

# display true hidden states
text(0, -1.2, adj = 0, cex = 0.8, col = "black", "True: green = A")
for (i in 1:n) {
  if (data$states[i] == "A")
    rect(i, -1, i + 1, 0, col = "green", border = NA)
  else rect(i, -1, i + 1, 0, col = "red", border = NA)
}

# display the post probable path
text(0, -3.2, adj = 0, cex = 0.8, col = "black", "Most probable path")
for (i in 1:n) {
  if (vit.out[i] == "A")
    rect(i, -3, i + 1, -2, col = "green", border = NA)
  else rect(i, -3, i + 1, -2, col = "red", border = NA)
}

# display the marginal most probable state
text(0, -5.2, adj = 0, cex = 0.8, col = "black", "Marginal most probable state")
for (i in 1:n) {
  if (mypost[1,i] > 0.5)
    rect(i, -5, i + 1, -4, col = "green", border = NA)
  else rect(i, -5, i + 1, -4, col = "red", border = NA)
}
```

### Fit the data with three hidden states

The number of hidden states is a tuning parameter. We can try a range of hidden states and then use AIC/BIC to select K, the number of hidden states.  

```{r}
set.seed(100)
tmpB = matrix(rgamma(n=9, shape=1), 3, 3)
tmpB = tmpB / rowSums(tmpB)
tmphmm = initHMM(c("A", "B", "C"), c(1, 2, 3), 
                 emissionProbs = tmpB)
print(tmphmm)
myfit3 = baumWelch(tmphmm, data$obs)
print(myfit3$hmm)
```

Note that the likelihood of the data (integrated over all possible hidden sequences) could be computed using the `forward` command. 

```{r}
# log-likelihood for K=2
f = forward(myfit2$hmm, data$obs) # f: 2xn
A = f[1,n]; B = f[2,n]
A + log(1 + exp(B-A))

# log-likelihood for K=3
f = forward(myfit3$hmm, data$obs) # f: 3xn
A = f[1,n]; B = f[-1,n]
A + log(1 + sum(exp(B-A)))
```
The two log-likelihoods are pretty close, so the model with two hidden states, which uses less number of parameters, will be chosen by AIC or BIC, once we add the penalty for the number of parametres.