
---
title: "Project 3 - Dictionary Building"
author: 
 - Tyler Zender (netID tzender2)
 - Matthew Lind (netID lind6)
 - Alejandro Pimentel (netID ap41,UIN 659282110)
date: '11/28/2022'
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: TRUE
    code_folding: show
    code_download: true
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Dictionary Building

## Data details

The data consists of 50,000 IMDB movie reviews, where each review is labelled as positive or negative.

- “id”, the identification number;
- “sentiment”, 0 = negative and 1 = positive;
- “score”, the 10-point score assigned by the reviewer. Scores 1-4 correspond to negative sentiment; Scores 7-10 correspond to positive sentiment. This data set contains no reviews with score 5 or 6.
- “review”.

```{r}
list.of.packages <- c("text2vec", "glmnet", "dplyr", "wordcloud")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```


```{r}
library(text2vec)
library(glmnet)
library(dplyr)
library(wordcloud)

train = read.table("alldata.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
```

To build the dictionary we focused on `review` and `sentiment`. We want a dictionary of terms that are significant for sentiment prediction. In the example below, the review has a positive sentiment.

```{r}
train[1,]$review
train[1,]$sentiment
```

## Preprocessing.

Some reviews contain undesired characters, below we remove some of them.

```{r}
train$review = gsub('<.*?>', ' ', train$review)
```

Below we use `text2vec` to create a vocabulary, the function `create_vocabulary` uses an iterator that tokenizes words/terms and change them to lower case.

`create_vocabulary` creates 2 main statistics for each term:

- term_count, number of occurrences of a term across all reviews.
- doc_count, number of reviews a term appears into.

A handful of terms are also defined as 'stop words' and are prevented from being used in our vocabulary.

```{r}
it_train = itoken(train$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)

stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "the", "us")

full.vocab = create_vocabulary(it_train, 
                      stopwords = stop_words, 
                              ngram = c(1L, 4L))
```

We prune the vocabulary with the following criteria:

- Terms must appear a minimum number of occurrences.
- Limit the maximum proportion of documents any term appears in.
- Limit the minimum proportion of documents any term appears in.

This gives us a more regulated and balanced usage of terms which prevents a few words from dominating our predictions.

```{r}
prune.vocab = prune_vocabulary(full.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
```

The number of terms after pruning is reduced from `r dim(full.vocab)[1]` to 
`r dim(prune.vocab)[1]`

## Vocabulary optimization.

First we need an appropriate matrix representation of the terms for each document/review.

We create a Document-Term Matrix (DTM), which has the reviews as rows and as columns the terms, each position in the matrix indicates how many times the term $j$ appears in the document $i$.

```{r}
dtm_train = create_dtm(it_train, vocab_vectorizer(prune.vocab))
```

```{r}
T_ = 50; 
max_terms = 1000
```


The resulting DTM matrix has dimensions: `r dim(dtm_train)`.

In the code below we produced the final vocabulary which should contain `1000` terms or less. To achieve this we perform the following steps:

1. Sample a subset of the terms in the DTM matrix.
2. Fit a `GLM` model with `Lasso` using the `sentiment` variable as a binary response.
3. Using $\lambda_{min}$ select the terms which coefficients $\hat\beta \neq 0$.

We repeat the procedure `r T_` times keeping a count of the times each fitted model selects each term. 

The goal is to increase the number of terms that are actually relevant for sentiment classification.

```{r}

# Initialize a data frame to keep count of term selections.
term_sel_count = data.frame(myvocab = "", freq = 0)

for( t in 1:T_){
  #print(paste("##### t =", t))
  uin_4 = 2110
  set.seed(uin_4 + t)  
  
  # randomly sample a subset of the data, e.g., 60%
  n = dim(dtm_train)[1]
  sample_idx = sample(1:n, n*0.6, replace=F)
  dtm_train_sub = dtm_train[sample_idx, ]
  y_subset = train$sentiment[sample_idx]
  
  # try lasso on this subset
  tmpfit = glmnet(x = dtm_train_sub, 
                  y = y_subset, 
                  alpha = 1,
                  family='binomial')
  
  # record selected variables using lambda.min
  idx = which.min(tmpfit$lambda)
  myvocab = colnames(dtm_train_sub)[which(tmpfit$beta[, idx] != 0)]
  sub_vocab = as_tibble(table(myvocab))
  
  term_sel_count = sub_vocab %>% 
    full_join(term_sel_count, by = "myvocab") %>%
    replace(is.na(.), 0) %>%
    mutate(freq = n + freq) %>%
    select(-c(n))
}

```

Finally, we select the top `r max_terms` terms. In other words, the terms that were selected the most and create the dictionary with those.

These terms are then written to our vocab file.

```{r}
term_sel_count = term_sel_count %>%
  arrange(desc(freq))

term_sel_count = term_sel_count[1:max_terms, ]

write(term_sel_count$myvocab, "myvocab.txt")
```

We can easily view our selected vocab terms as well as the frequency of those terms in a word cloud visualization, where the size and color of each word correlates to the frequency. This visualization allows for an easy 'sanity check' of our vocabulary since we would ideally expect all the words to be terms that carry positive or negative sentiments, though some of the larger words may be those which are less outright positive or negative and are simply just more commonly used.

```{r}
terms_freqs = term_sel_count %>% 
  left_join(prune.vocab, by = c("myvocab" = "term")) 

wordcloud(words = terms_freqs$myvocab, freq = terms_freqs$term_count, max.words = 300, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```

## Adittional code

The code below corresponds to additional models we tried and analyze in the final report. 

```{r eval=FALSE, class.source = 'fold-hide'}
library( text2vec )
library( glmnet )
require( xgboost )

#####################################
# Load libraries
# Load your vocabulary and training data
#####################################
myvocab <- scan( file = "myvocab3.txt", what = character() )
train   <- read.table( "train.tsv", stringsAsFactors = FALSE, header = TRUE )

# train$review <- gsub( "[\'\"]", "", train$review )   # remove contractions/possessives
train$review <- gsub( '<.*?>', ' ', train$review )     # remove HTML tags
if ( regexp != "" ) {
  train$review <- gsub( regexp, ' ', train$review )   # other
}

it_train   = itoken( train$review, preprocessor = tolower, tokenizer = word_tokenizer )
vectorizer = vocab_vectorizer( create_vocabulary( myvocab, ngram = n_gram ) )
dtm_train  = create_dtm( it_train, vectorizer )

#####################################
# Train a binary classification model
#####################################

NFOLDS = nb_folds
t1     = Sys.time()

if ( loss_type == 2 ) {
  # Gradient boosting tree
  xgboost_classifier <- xgboost(
    data      = as.matrix( dtm_train ),
    label     = train$sentiment,
    max_depth = 4,
    eta       = 0.115,
    nrounds   = max_iterations,
    subsample = sample_rate,
    verbose   = FALSE
  )
} else {
  # ridge/lasso regularization
  glmnet_classifier = cv.glmnet(
    x            = dtm_train,
    y            = train$sentiment,
    family       = 'binomial',
    alpha        = loss_type,       # 0=L2 penalty (Ridge), 1=L1 penalty (lasso)
    type.measure = "auc",           # interested in area under ROC curve
    nfolds       = NFOLDS,          # 4-fold cross-validation
    thresh       = threshold,       # high value is less accurate, but faster training
    maxit        = max_iterations   # again lower number of iterations for faster training
  )
}

d_time = difftime( Sys.time(), t1, units = 'sec' )
print( d_time )
results[j,1] = d_time

#####################################
# Load test data, and 
# Compute prediction
#####################################
test <- read.table( "test.tsv", stringsAsFactors = FALSE, header = TRUE )

# test$review = gsub( "[\'\"]", "", test$review )   # remove contractions/possessives
test$review <- gsub( '<.*?>', ' ', test$review )    # remove HTML tags
if ( regexp != "" ) {
  test$review <- gsub( regexp, ' ', test$review )   # other
}

it_test    = itoken( test$review, preprocessor = tolower, tokenizer = word_tokenizer )
vectorizer = vocab_vectorizer( create_vocabulary( myvocab, ngram = n_gram ) )
dtm_test   = create_dtm( it_test, vectorizer )

if ( loss_type == 2 ) {
  # gradient boosting tree
  preds = predict( xgboost_classifier, dtm_test, type = 'response' )
} else {
  # Lasso/ridge regression
  preds = predict( glmnet_classifier, dtm_test, type = 'response' )[,1]
}

output = cbind( test$id, preds )
output = as.data.frame( output )
colnames( output ) = c( 'id', 'prob' )

#####################################
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predicted probs
#####################################
write.table( output, file = "mysubmission.txt", row.names = FALSE, sep='\t' )

```

The code below is for evaluation.

```{r eval=FALSE, class.source = 'fold-hide'}
library( pROC )

set.seed( 867 )

# global variables
results        = matrix( 0, 5, 2 )
regexp         = "" # [^a-zA-Z0-9]"
loss_type      = 0    # 0=ridge, 1=lasso, 2=XGBoost
sample_rate    = 0.55
n_gram         = c(1L, 4L )
nb_folds       = 4
threshold      = 1e-3
max_iterations = 1e3

min_auc = 1

top_dir = getwd( )
mymain  = paste( top_dir, "/mymain.R", sep = '' )

tt = Sys.time()
str = NULL
for ( j in 1:5 )
{
  cat( str, "--- split ", j, " ---\n", sep="" )
  #print( paste( "J:", j ) )
  
  # Get the current split's working directory path 
  wd = paste( getwd( ), "/split_", j, sep = '' )
  
  # Set directory to the current split's folder
  setwd( wd )
  
  file.copy( file.path( top_dir, "myvocab3.txt" ), wd, overwrite = TRUE )
  
  source( mymain )
  
  # move "test_y.tsv" to this directory
  test.y  <- read.table( "test_y.tsv",       header = TRUE )
  pred    <- read.table( "mysubmission.txt", header = TRUE )
  pred    <- merge( pred, test.y, by="id" )
  roc_obj <- roc( pred$sentiment, pred$prob )
  this_split_auc = pROC::auc( roc_obj )
  #print( paste( "split AUC:", this_split_auc ) )
  cat( str, "AUC: ", this_split_auc, "\n", sep="" )
  results[j,2] = this_split_auc
  min_auc = min( min_auc, this_split_auc )
  setwd( '..' )
}
#print( paste( "Minimum AUC: ", min_auc ) )
d_time = difftime( Sys.time(), tt, units = 'sec' )

#------------------------------------------------
cat( str, "--- results ---",
     "\n Loss Type: ", loss_type, 
     "\n    N-gram: ", n_gram[1], " ", n_gram[2], 
     "\n   NbFolds: ", nb_folds,
     "\n threshold: ", threshold, 
     "\niterations: ", max_iterations, 
     "\n    regexp: ", regexp, 
     "\n",
     "\n", results[1,1], " sec   AUC: ", results[1,2],
     "\n", results[2,1], " sec   AUC: ", results[2,2],
     "\n", results[3,1], " sec   AUC: ", results[3,2],
     "\n", results[4,1], " sec   AUC: ", results[4,2],
     "\n", results[5,1], " sec   AUC: ", results[5,2],
     "\n\n", d_time, " sec   min AUC: ", min_auc, 
     "\n", 
     sep=""
)

```

