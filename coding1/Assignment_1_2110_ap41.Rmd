---
title: "Coding 1 - PSL 598, Fall 2022"
author: "Alejandro Pimentel (netID ap41,UIN 659282110)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: TRUE
    code_folding: show
    code_download: true
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{css, class.source = 'fold-hide'}
p, li, td {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
```

```{r}
# Set a seed to have a stable generation of random numbers
uin_4 = 2110
set.seed(uin_4)
```


# Generate Centers

Generate the 20 centers as follows: the 10 centers associated with class 1 are from one normal distribution and the 10 centers associated with class 0 from another normal distribution, 
$$\mathbf{m}_{1j} \sim \mathcal{N} \Big ( \left ( \begin{array}{c} 1 \\ 0 \end{array} \right ), \mathbf{I}_2 \Big ), \quad \mathbf{m}_{0j} \sim \mathcal{N} \Big ( \left ( \begin{array}{c} 0 \\ 1 \end{array} \right ), \mathbf{I}_2 \Big ), \quad j=1:10. $$

```{r message=FALSE, warning=FALSE}

library(ggplot2)
library(class)

p = 2;      
csize = 10;     # number of centers
sigma = 1;      # sd for generating the centers 
m1 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind( rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind( rep(0, csize), rep(1, csize))
```

# Generate Data

You can use `generate_sim_data` below to return a training sample of size 200 and a test sample of size 10,000, given generated centers $\mathbf{m}_{kj}$.

```{r message=FALSE, warning=FALSE}
sim_params = list(
 csize = 10,      # number of centers
 p = 2,           # dimension
 s = sqrt(1/5),   # standard deviation for generating data
 n = 100,         # training size per class
 N = 5000,        # test size per class
 m0 = m0,         # 10 centers for class 0
 m1 = m1         # 10 centers for class 1
)

generate_sim_data = function(sim_params){
  p = sim_params$p
  s = sim_params$s 
  n = sim_params$n 
  N = sim_params$N 
  m1 = sim_params$m1 
  m0 = sim_params$m0
  csize = sim_params$csize
  
  id1 = sample(1:csize, n, replace = TRUE);
  id0 = sample(1:csize, n, replace = TRUE);
  Xtrain = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
  Ytrain = factor(c(rep(1,n), rep(0,n)))
  id1 = sample(1:csize, N, replace=TRUE);
  id0 = sample(1:csize, N, replace=TRUE); 
  Xtest = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id1,], m0[id0,])
  Ytest = factor(c(rep(1,N), rep(0,N)))
  
  
  # Return the training/test data along with labels
  list(
  Xtrain = Xtrain,
  Ytrain = Ytrain,
  Xtest = Xtest,
  Ytest = Ytest
  )
}
```

```{r}
mydata = generate_sim_data(sim_params)
```

# Visualization

The following graph shows the generated training data for the 2 classes. The 10 centers used for the data genearation are
also shown as '+' symbols.

```{r warning=FALSE, class.source = 'fold-hide'}
tmp.X = mydata$Xtrain
tmp.Y = mydata$Ytrain
m0 = sim_params$m0        # 10 centers for class 0
m1 = sim_params$m1  

n = nrow(tmp.X)
mycol = rep("blue", n)
mycol[tmp.Y == 0] = "red"
plot(tmp.X[, 1], tmp.X[, 2], type = "n", xlab = "", ylab = "")
points(tmp.X[, 1], tmp.X[, 2], col = mycol);
points(m1[, 1], m1[, 2], pch = "+", cex = 2, col = "blue");    
points(m0[, 1], m0[, 2], pch = "+", cex = 2, col = "red");   
legend("bottomright", pch = c(1,1), col = c("red", "blue"), 
       legend = c("class 0", "class 1")) 
```

# Part I: KNN

The following function performs predictions on a test data set using k Nearest Neighbors 
algorithm. Roughly speaking, the algorithm follows these steps for each test point:

- Calculates the euclidean distance from a test point to all training points.
- Takes the k closest training points. The order function will preserve the order of appearance in case of distance tie.
- It takes the most repeated class among the k closest points as the prediction for the test point. If there is voting tie,
  i.e. both classes appeared the same number of times, the algorithm takes class 0. This is a side effect of using, `table` and
  `which.max` functions.

```{r}
my_knn = function(traindata, testdata, Ytrain, k) {
  
  Ypred <- vector(mode = 'integer', length = dim(testdata)[1])
  
  for (i in 1:dim(testdata)[1]) {
    dist = rowSums(sweep(traindata, 2, testdata[i,])^2)
    k_nearest_classes = cbind(dist, Ytrain)[order(dist)[1:k], , drop=F]
    k_nearest_classes = data.frame(k_nearest_classes)
    
    voted_max = as.integer(names(which.max(table(k_nearest_classes$Ytrain)))[1])
    Ypred[i] = voted_max - 1
  }
  
  # convert perdictions to factors
  factor(Ypred)
}


# declare some shorcut variables for next sections
traindata = mydata$Xtrain
testdata = mydata$Xtest
Ytrain = mydata$Ytrain
Ytest = mydata$Ytest
N = sim_params$N
```

## Handling distance and voting ties.

When `k` is an odd number the algorithm does not have voting ties.
When `k` is even, I simply select the first class from the max operation: `which.max(table(k_nearest_classes$Ytrain))`

Similarly, for distance ties, I take the `k` smallest distances according to R's `order` function.

See an example below when `k=4` and there is a voting tie.

```{r}
k=4
diff_idx = 502
diff_point = testdata[diff_idx, , drop=FALSE]

dist = rowSums(sweep(traindata, 2, diff_point)^2)
k_nearest_classes = cbind(dist, Ytrain)[order(dist)[1:k], , drop=F]
(k_nearest_classes = data.frame(k_nearest_classes))
table(k_nearest_classes$Ytrain)
```

The table operation above orders the columns using the class names. Then the max operation below takes the first max it finds.

```{r}
names(which.max(table(k_nearest_classes$Ytrain)))[1]
```


## Using my slow knn

Below we make predictions for 3 values of `k`, using first our algorithm and then R's `knn`.

```{r}
# Predictions
test.pred.k1 = my_knn(traindata, testdata, Ytrain, k = 1)
test.pred.k3 = my_knn(traindata, testdata, Ytrain, k = 3)
test.pred.k5 = my_knn(traindata, testdata, Ytrain, k = 5)
# Errors
test.err.k1 = sum(Ytest != test.pred.k1)/(2*N)
test.err.k3 = sum(Ytest != test.pred.k3)/(2*N)
test.err.k5 = sum(Ytest != test.pred.k5)/(2*N)
```

- Average loss when $k=1$, `r test.err.k1`
- Average loss when $k=3$, `r test.err.k3`
- Average loss when $k=5$, `r test.err.k5`

## Using knn from class lib

```{r}
# Predictions
test.pred.ck1 = knn(traindata, testdata, Ytrain, k = 1)
test.pred.ck3 = knn(traindata, testdata, Ytrain, k = 3)
test.pred.ck5 = knn(traindata, testdata, Ytrain, k = 5)
# Errors
test.err.ck1 = sum(Ytest != test.pred.ck1)/(2*N)
test.err.ck3 = sum(Ytest != test.pred.ck3)/(2*N)
test.err.ck5 = sum(Ytest != test.pred.ck5)/(2*N)
```

- Average loss when $k=1$, `r test.err.ck1`
- Average loss when $k=3$, `r test.err.ck3`
- Average loss when $k=5$, `r test.err.ck5`

## Comparing kNN predictions with contingency tables

First we compare our algorithm predictions with the ground truth `Ytest`.

```{r}
table(Ytest, test.pred.k1)
table(Ytest, test.pred.k3)
table(Ytest, test.pred.k5)
```

Below we are comparing predictions between the 2 algorithms.

```{r}
table(test.pred.k1, test.pred.ck1)
table(test.pred.k3, test.pred.ck3)
table(test.pred.k5, test.pred.ck5)
```

When `k=3` there is discrepancy due to the distance tolerance the `kNN` algorithm uses. In this case the 3th and 4th distances for the test point in question are very close (as shown below), `kNN` then includes the 4th distance in the voting. So for `kNN`, the probability for the classes is tied (i.e. 0.5) in such cases.

```{r}
# We set k to 4 so we can show the distance proximity between point 3 and 4.
k=4
# Looking for point with discrepancy.
diff_idx = which(test.pred.k3 != test.pred.ck3)
diff_point = testdata[diff_idx, , drop=FALSE]

dist = rowSums(sweep(traindata, 2, diff_point)^2)
k_nearest_classes = cbind(dist, Ytrain)[order(dist)[1:k], , drop=F]
data.frame(k_nearest_classes)
```

# Part II: cv-KNN

## Compute CV Errors

The function below computes the Cross Validation Error for a given `k`.

```{r}

compute_cv_error <- function(traindata, Ytrain, k = 1, foldNum = 10){

  n = nrow(traindata)
  fold_size = floor(n/foldNum)
  error = 0
  random_idx = sample(1 : n)
  for(runId in 1:foldNum){
    # this line produces sets like (when foldNum=10 and n=100): 1:10, 11:20 ... 91:100
    testSetIndex = ((runId-1)*fold_size + 1):(ifelse(runId == foldNum, n, runId*fold_size))
    testSetIndex = random_idx[testSetIndex]
    trainX = traindata[-testSetIndex, ]
    trainY = Ytrain[-testSetIndex]
    testX = traindata[testSetIndex, ]
    testY = Ytrain[testSetIndex]
    predictY = knn(trainX, testX, trainY, k)
    error = error + sum(predictY != testY) 
  }
  error = error / n
  error
}

compute_cv_error(traindata, Ytrain, k = 1)

```


## Find Best K

The function below iterates a set of values `k` to perform Cross Validation on each.

- The function then selects the `k` with the smallest error.
- If there is a tie (optimal `k` is not unique), the greater `k` value is picked.

```{r}

cvKNN <- function(traindata, Ytrain, k_candidates, foldNum) {
  n = nrow(traindata)
  cvErrorRates = rep(NaN, length(k_candidates))
  
  for(i in 1:length(k_candidates)) {
    cvErrorRates[i] = compute_cv_error(traindata, Ytrain, k_candidates[i])
  }
  
  result = list()
  result$bestK = max(k_candidates[cvErrorRates == min(cvErrorRates)])
  result$cvError = min(cvErrorRates)
  result
}


```

Below we test our function for 180 `k` values.

```{r}
cvKNN(traindata, Ytrain, seq(1, 180), 10)
```


# Part III: Bayes Rule

## Implement the Bayes Rule.

Using `Bayes Rule` we predict $Y$ for a given test point $x$.

```{r}

BayesPredict = function(x, sim_params){
  
  m1 = sim_params$m1
  m0 = sim_params$m0
  s = sim_params$s
  d1 = sum(exp(-apply((t(m1) - x)^2, 2, sum) / (2 * s^2)))
  d0 = sum(exp(-apply((t(m0) - x)^2, 2, sum) / (2 * s^2))) 
  
  br = d1 / d0
  ifelse(br >= 1, 1, 0)
}

# Testing on a single point
BayesPredict(testdata[200, ], sim_params)
```

## Decision Boundary (Optional)

```{r}
Xmin = min(mydata$Xtrain[, 1])
Xmax = max(mydata$Xtrain[, 1])
X1Vector = seq(Xmin, Xmax, (Xmax - Xmin)/99)
Xmin = min(mydata$Xtrain[, 2])
Xmax = max(mydata$Xtrain[, 2])
X2Vector = seq(Xmin, Xmax, (Xmax - Xmin)/99)
grid = expand.grid(X1Vector, X2Vector)
colnames(grid) = c('X1', 'X2')
```

```{r}
BayesRuleGrid = grid
BayesRuleGrid$Y = as.factor(apply(BayesRuleGrid, 1, BayesPredict, sim_params))

tmp.data = data.frame(X1 = c(sim_params$m0[,1], sim_params$m1[,1]),
                      X2 = c(sim_params$m0[,2], sim_params$m1[,2]),
                      Y = c(rep(0, 10), rep(1, 10)))
ggplot(data = tmp.data, aes(x = X1, y = X2, color = as.factor(Y))) + 
  geom_point(shape = 3, size = 3) +
  geom_point(data = BayesRuleGrid, 
             aes(x = X1, y = X2, color = Y), size = 0.1, alpha = 0.1) + 
  scale_color_manual(name = "Y",
                     values = c("0" = "red",
                                "1" = "blue"), 
                     labels = c("class 0", "class 1"))
```

# Part IV: Simulation Study

In this part we made predictions over 50 generated datasets using 3 methods:

- kNN when `k=1`.
- `k` value picked by the Cross Validation function `cvKNN` defined above.
- Using Bayes Rule.

```{r}
N = sim_params$N

num_datasets = 50
# pre-allocate vectors to record the prediction error on each data set.
test.errors.k1 = rep(NaN, num_datasets)
test.cv = list(
  errors = rep(NaN, num_datasets),
  ks = rep(NaN, num_datasets)
)
test.errors.bayes = rep(NaN, num_datasets)

for(i in 1:num_datasets) {
  dataset = generate_sim_data(sim_params)
  
  # Predictions and error for kNN when k=1
  preds_k1 = knn(dataset$Xtrain, dataset$Xtest, dataset$Ytrain, k = 1)
  test.errors.k1[i] = sum(dataset$Ytest != preds_k1)/(2*N)
  
  # Predictions and error for kNN when k is picked via CV
  ks_cv = cvKNN(dataset$Xtrain, dataset$Ytrain, seq(1, 180), 10)
  preds_selected_k = knn(dataset$Xtrain, dataset$Xtest, dataset$Ytrain, k = ks_cv$bestK)
  test.cv$errors[i] = sum(dataset$Ytest != preds_selected_k)/(2*N)
  test.cv$ks[i] = ks_cv$bestK
  
  # Predictions using Bayes Rule
  preds_bayes = as.factor(apply(dataset$Xtest, 1, BayesPredict, sim_params))
  test.errors.bayes[i] = sum(dataset$Ytest != preds_bayes)/(2*N)
}
```


## Comparing test errors.

The following `boxplot` shows how the errors for each method are distributed.

- `Bayes rules` here represents our ideal (unattainable) predictor based on the known centers $m0$ and $m1$.
- `k selected via CV` performs better on the test set than simply setting $k=1$.

```{r}
boxplot(test.errors.k1, test.cv$errors, test.errors.bayes,
             names = c("kNN with k=1", "k chosen with 10-fold CV", "Bayes rule"),
             xlab = "Prediction algorithms used.",
             ylab = "Averaged 0/1 loss",
             main = "Comparing prediction errors on 50 simulated datasets.",
             pch = 20,
             cex = 2,
             border = "brown",
             col = "lightblue")
```

## k values chosen by 10-fold CV

The following bar chart displays best 50 `k` values selected by `10-fold cross-validation kNN` classifier.

```{r}
ggplot(as.data.frame(test.cv), aes(x=1:num_datasets, y=ks)) + 
    geom_bar(stat="identity",fill="steelblue4") + theme_minimal() +
    geom_text(aes(label=ks), position=position_dodge(width=0.9), vjust=-0.25) +
    xlab("Simulation ID") + ylab("Selected Best k")
```


The mean of the selected k values:

```{r}
mean(test.cv$ks)
```


The standard deviation of the selected k values:

```{r}
sd(test.cv$ks)
```













