---
title: "Coding 4 - PSL 598, Fall 2022"
author: "Alejandro Pimentel (netID ap41,UIN 659282110)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: TRUE
    code_folding: show
    code_download: true
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

# PART I

> Implement the EM algorithm for a $p$-dimensional Gaussian mixture model with $G$ components:

$$
p(x \mid p_{1:G}, \mu_{1:G}, \Sigma) = \sum_{k=1}^G p_k \cdot N(x; \mu_k, \Sigma).
$$

## Prepare your function

You should write a function to perform the E-step, a function to perform the M-step, and then iteratively call these two functions in `myEM`. 

You also need to prepare
a function to evaluate the loglikelihood; call that function after you finish the EM iterations. 


```{r}
Estep <- function(data, G, para){
  # Your Code
  # Return the n-by-G probability matrix
  X = data
  A = t(X)
  p = dim(X)[2]
  n = nrow(X)
  r_nG = matrix(0, n, G)
  inv_S = solve(para$Sigma)
  det_S = det(para$Sigma)
  
  for (k in 1:G) {
    A_mu_diff = A - para$mean[,k]
    # What I think is the density for component k
    g_den = exp(-1/2 * colSums((inv_S %*% A_mu_diff) * A_mu_diff)) / sqrt((2*pi)^p * det_S)
    r_n = para$prob[k] * g_den
    r_nG[,k]= r_n
  }
  
  # Normalize
  Z = r_nG / rowSums(r_nG)  
  return(Z)
}

Mstep <- function(data, G, para, post.prob){ 
  # Your Code
  # Return the updated parameters
  para.new <- list(prob = NULL, 
                   mean = NULL, 
                   Sigma = NULL, 
                   loglik = NULL)
  
  X = as.matrix(data)
  A = t(X)
  n = nrow(X)
  p = dim(X)[2]
  
  # new prob
  para.new$prob = apply(post.prob, 2, mean)  
  
  # new mu
  norm_w = colSums(post.prob)  
  para.new$mean = t(t(A %*% post.prob) / norm_w)
  
  # new Sigma
  c = matrix(0, p, p)
  for (k in 1:G) {
    # R works column by column, this is easier with python
    X_ = t(A - para.new$mean[,k])
    # my brain does not understand this diagonal trick
    c =  c + (t(X_) %*% diag(post.prob[,k]) %*% X_)
  }
  
  para.new$Sigma = c / n
  
  return(para.new)  
}

loglik <- function(data, G, para){
	# compute loglikelihood
  X = data
  A = t(X)
  p = dim(X)[2]
  inv_S = solve(para$Sigma)
  det_S = det(para$Sigma)
  loglikeli = 0
  
  for (i in 1:n) {
    logli_k = 0
    for (k in 1:G) {
      a = as.matrix(X[i, ] - para$mean[,k])
      b = exp(a %*% inv_S %*% t(a) / -2) / sqrt((2*pi)^p * det_S)
      logli_k = logli_k + (para$prob[k]  * b) 
    }
    loglikeli = loglikeli + log(logli_k)
  }
  
  return(loglikeli)
}

myEM <- function(data, itmax, G, para){
  # itmax: number of of iterations
  # G:     number of components
  # para:  list of (prob, mean, Sigma, loglik)
  
  for(t in 1:itmax){
    post.prob <- Estep(data, G, para)
    para <- Mstep(data, G, para, post.prob)
  }
  
  # update para$loglik
  para$loglik = loglik(data, G, para)
  
  return(para)
}
```



## Test your function


Test your function on the `faithful` data with $G = 2$ and $G = 3$ with **20** iterations (i.e., `itmax = 20`). The mixture model described above corresponds to `modelName = "EEE"` in `mclust` (you do not need to know why).



Set the number of printing digits to be eight. The result from your function should agree with the result from `mclust`, up to the speciofied precision level. 

```{r}
options(digits=8)
```


### Load data

Load the `faithful` data from R package `mclust`.
```{r}
library(mclust)
dim(faithful)
head(faithful)
```


### Two clusters

Compare the result returned by `myEM` and the one returned by the EM algorithm in `mclust` after 20 iterations.


We **initialize** parameters by first randomly assigning the $n$ samples into two groups and then running one iteration of the built-in M-step. 

```{r}
n <- nrow(faithful)
G <- 2
uin_4 = 2110
set.seed(uin_4)
gID <- sample(1:G, n, replace = TRUE)
Z <- matrix(0, n, G)
for(k in 1:G)
  Z[gID == k, k] <- 1 
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
```

Here are the initial values we use for (prob, mean, Sigma).

```{r}
para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma, 
              loglik = NULL)
```


* Output from `myEM`

```{r}
myEM(data=faithful, itmax=20, G=G, para=para0)
```

* Output from `mclust`
```{r}
Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 20), 
           parameters = ini0)
list(prob = Rout$para$pro, mean = Rout$para$mean, 
     Sigma = Rout$para$variance$Sigma, 
     loglik = Rout$loglik)
```


### Three clusters

Similarly, set $G=3$, then compare the result returned by `myEM` and the one returned by the em algorithm from `mclust` after 20 iterations.

```{r}
n <- nrow(faithful)
G <- 3
uin_4 = 2110
set.seed(uin_4)
gID <- sample(1:G, n, replace = TRUE)
Z <- matrix(0, n, G)
for(k in 1:G)
  Z[gID == k, k] <- 1 
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
```

Here are the initial values we use for (prob, mean, Sigma).

```{r}
para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma, 
              loglik = NULL)
```

* Output from `myEM`

```{r}
myEM(data=faithful, itmax=20, G=G, para=para0)
```

* Output from `mclust`
```{r}
Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 20), 
           parameters = ini0)
list(prob = Rout$para$pro, mean = Rout$para$mean, 
     Sigma = Rout$para$variance$Sigma, 
     loglik = Rout$loglik)
```

## Derivation

Partial results for the required expressions are given below. 

1. Expression of the marginal (or the so-called incomplete) likelihood function  or its log, which is the objective function we aim to maximize.


$$
\begin{aligned}
& \prod_{i=1}^n  p(x_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \prod_{i=1}^n  \big[   p_1 N(x_i; \mu_1, \Sigma) + \cdots + p_G N(x_i; \mu_G, \Sigma) \big ]\\
= & \prod_{i=1}^n  \Big [ p_1  \frac{\exp  ( - \frac{1}{2} (x- \mu_1)^t \Sigma^{-1} (x - \mu_1)  )}{\sqrt{(2 \pi)^p | \Sigma| }}
 + \cdots + p_G \frac{\exp  ( - \frac{1}{2} (x- \mu_G)^t \Sigma^{-1} (x - \mu_G)  )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]
\end{aligned}
$$
where $|\Sigma|$ denotes the determinant of matrix $\Sigma$.

2. Expression of the complete likelihood function $\sum_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma)$ or its log, which is the function we work with in the EM algorithm.

$$
\begin{aligned}
& \prod_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \prod_{i=1}^n \prod_{k=1}^G   \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]^{\mathbb{1}_{\{Z_i = k \}}}
\end{aligned}
$$

3. Expression of the distribution of $Z_i$ at the E-step. 

    Given data and the current parameter value$(p_{1:G}^{(0)}, \mu_{1:G}^{(0)}, \Sigma^{(0)})$, $Z_i$ follows  a discrete distribute taking values from $1$ to $G$ with probabilities
$$
\begin{aligned}
p_{ik} := & \mathbb{P}(Z_i = k \mid x_i, p_{1:G}^{(0)}, \mu_{1:G}^{(0)}, \Sigma^{(0)}) \\
= & \text{ your expression}
\end{aligned}
$$

4. Expression of the objective function you aim to maximize (or minimize) at the M-step.

    At the M-step, we optimize the following objective function (where the expectation is taken over $Z_1, \dots, Z_n$ with respect to the probabilities computed at step 3): 
    
$$   
\begin{aligned}
g(p_{1:G}, \mu_{1:G}, \Sigma) = & \mathbb{E} \log \prod_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \mathbb{E} \sum_{i=1}^n \sum_{k=1}^G  \mathbb{1}_{\{Z_i = k \}} \log  \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ] \\
= & \sum_{i=1}^n \sum_{k=1}^G  p_{ik}  \log  \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]

\end{aligned}
$$

where the last step is due to the fact that $\mathbb{E} [ \mathbb{1}_{\{Z_i = k \}}] = \mathbb{P}(Z_i = k) = p_{ik}.$

5. Derivation and the updating formulas for $p_{1:G}, \mu_{1:G}, \Sigma$ at the M-step.

# PART II

## The Baum-Welch  algorihtm

The Baum-Welch Algorithm is the EM algorithm for HMM. You should prepare a function `BW.onestep` to perform the E-step and M-step, and then iteratively call that function in `myBW`.

**Note that We do not update w.**

```{r}
myBW = function(x, para, n.iter = 100){
  # Input:
  # x: T-by-1 observation sequence
  # para: initial parameter value
  # Output updated para value (A and B; we do not update w)
  
  for(i in 1:n.iter){
    para = BW.onestep(x, para)
  }
  return(para)
}
```

Your function `BW.onestep`, in which we operate the E-step and M-step for one iteration, should look as follows. 

```{r}
BW.onestep = function(x, para){
  # Input: 
  # x: T-by-1 observation sequence
  # para: mx, mz, and current para values for
  #    A: initial estimate for mz-by-mz transition matrix
  #    B: initial estimate for mz-by-mx emission matrix
  #    w: initial estimate for mz-by-1 initial distribution over Z_1
  # Output the updated parameters after one iteration
  # We DO NOT update the initial distribution w
  
  T = length(x)
  mz = para$mz
  mx = para$mx
  A = para$A
  B = para$B
  w = para$w
  alp = forward.prob(x, para)
  beta = backward.prob(x, para)
  
  myGamma = array(0, dim=c(mz, mz, T-1))
  #######################################
  ## YOUR CODE: 
  ## Compute gamma_t(i,j) P(Z[t] = i, Z[t+1]=j), 
  ## for t=1:T-1, i=1:mz, j=1:mz, 
  ## which are stored in an array, myGamma
  #######################################
  for(t in 1:(T-1)) {
    for(i in 1:mz) {
      for(j in 1:mz) {
        logs_ = log(c(alp[t, i], A[i, j], B[j, x[t+1]], beta[t+1, j]))
        myGamma[i, j, t] = exp(sum(logs_))
      }
    }
  }
  
  # M-step for parameter A
  #######################################
  ## YOUR CODE: 
  ## A = ....
  #######################################
  newA = matrix(0, mz, mz)
  
  # sum all the mz by mz matrices
  for(t in 1:(T-1)) {
    newA = newA + myGamma[,,t]
  }
  # Convert to probability vectors for each Zi
  newA = newA / rowSums(newA)
  
  # M-step for parameter B
  #######################################
  ## YOUR CODE: 
  ## B = ....
  #######################################
  
  newB = matrix(0, mz, mx)
  #i=1;l=2
  for (i in 1:mz) {
    for (l in 1:mx) {
      Ts = which(x == l)
      if (any(Ts==T)) {
        newB[i,l] = sum(myGamma[,i,T-1]) 
      }
      Ts = Ts[Ts != T]
      newB[i,l] = newB[i,l]+sum(myGamma[i,,Ts]) 
    }
  }
  
  newB = newB / rowSums(newB)
  
  para$A = newA
  para$B = newB
  return(para)
}
```

You can compute the forward and backward probabilities using the following functions.

```{r}
forward.prob = function(x, para){
  # Output the forward probability matrix alp 
  # alp: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  alp = matrix(0, T, mz)
  
  # fill in the first row of alp
  alp[1, ] = w * B[, x[1]]
  # Recursively compute the remaining rows of alp
  for(t in 2:T){
    tmp = alp[t-1, ] %*% A
    alp[t, ] = tmp * B[, x[t]]
    }
  return(alp)
}

backward.prob = function(x, para){
  # Output the backward probability matrix beta
  # beta: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  beta = matrix(1, T, mz)

  # The last row of beta is all 1.
  # Recursively compute the previous rows of beta
  for(t in (T-1):1){
    tmp = as.matrix(beta[t+1, ] * B[, x[t+1]])  # make tmp a column vector
    beta[t, ] = t(A %*% tmp)
    }
  return(beta)
}
```

## The Viterbi algorihtm

The Viterbi algorihtm returns the most probable latent sequence given the data and the MLE of parameters. 

```{r}
myViterbi = function(x, para){
  # Output: most likely sequence of Z (T-by-1)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  log.A = log(A)
  log.w = log(w)
  log.B = log(B)
  
  # Compute delta (in log-scale)
  delta = matrix(0, T, mz) 
  # fill in the first row of delta
  delta[1, ] = log.w + log.B[, x[1]]
  
  #######################################
  ## YOUR CODE: 
  ## Recursively compute the remaining rows of delta
  #######################################

  for (t in 2:T) {
    for (i in 1:mz) {
      delta[t, i] = log.B[i, x[t]] + max(delta[t - 1, ] + log.A[, i])
    }
  }
  
  # Compute the most prob sequence Z
  Z = rep(0, T)
  # start with the last entry of Z
  Z[T] = which.max(delta[T, ])
  
  #######################################
  ## YOUR CODE: 
  ## Recursively compute the remaining entries of Z
  #######################################

  for (t in (T - 1):1) {
    Z[t] = which.max(delta[t, ] + log.A[, Z[t + 1]] )
  }
  
  return(Z)
}
```

## Test your function

### Baum-Welch results
Try your code on the data provided on Campuswire. You can (i) use the initial values specified below or (ii) use your own initial values. For the latter, remember to set the seed as the last four digits of your UIN. 

```{r}
data = scan("coding4_part2_data.txt")

mz = 2
mx = 3
ini.w = rep(1, mz)
ini.w = ini.w / sum(ini.w)
ini.A = matrix(1, 2, 2)
ini.A = ini.A / rowSums(ini.A)
ini.B = matrix(1:6, 2, 3)
ini.B = ini.B / rowSums(ini.B)
ini.para = list(mz = 2, mx = 3, w = ini.w,
                A = ini.A, B = ini.B)

myout = myBW(data, ini.para, n.iter = 100)


# Calling library HMM
library(HMM)
hmm0 =initHMM(c("A", "B"), c(1, 2, 3),
              startProbs = ini.w,
              transProbs = ini.A, 
              emissionProbs = ini.B)
Rout = baumWelch(hmm0, data, maxIterations=100, delta=1E-9, pseudoCount=0)

```

```{r}
options(digits=8)
```

- Compare estimates for transition prob matrix A
```{r}
myout$A
Rout$hmm$transProbs
```

- Compare estimates for emission prob matrix B
```{r}
myout$B
Rout$hmm$emissionProbs
```

### Viterbi results

```{r}

# Using output  from library as per instructions to reduce risk of double penalization.
para = list(mz = mz, mx = mx, w = ini.w,
            A = Rout$hmm$transProbs, B = Rout$hmm$emissionProbs)

myout.Z = myViterbi(data, para)
myout.Z[myout.Z==1] = 'A'
myout.Z[myout.Z==2] = 'B'

# Calling library
Rout.Z = viterbi(Rout$hmm, data)
```

- Compare the most probable Z sequence.
```{r}
cbind(Rout.Z, myout.Z)[c(1:10, 180:200), ]
sum(Rout.Z != myout.Z)
```

