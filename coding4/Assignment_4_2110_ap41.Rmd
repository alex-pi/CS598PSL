---
title: "Coding 4 - PSL 598, Fall 2022"
author: "Alejandro Pimentel (netID ap41,UIN 659282110)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: TRUE
    code_folding: show
    code_download: true
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

# PART I

Implementation of the `EM` algorithm for a $p$-dimensional Gaussian mixture model with $G$ components:

$$
p(x \mid p_{1:G}, \mu_{1:G}, \Sigma) = \sum_{k=1}^G p_k \cdot N(x; \mu_k, \Sigma).
$$

## Function definitions

Below we define a function to perform the E-step, a function to perform the M-step, and then iteratively call these two functions in `myEM`. 

We also prepare a function to evaluate the loglikelihood; which we call after the EM iterations. 


```{r}
Estep <- function(data, G, para){

  # Return the n-by-G probability matrix
  X = data
  A = t(X)
  p = dim(X)[2]
  n = nrow(X)
  r_nG = matrix(0, n, G)
  inv_S = solve(para$Sigma)
  det_S = det(para$Sigma)
  
  for (k in 1:G) {
    A_mu_diff = A - para$mean[,k]
    # What I think is the density for component k
    g_den = exp(-1/2 * colSums((inv_S %*% A_mu_diff) * A_mu_diff)) / sqrt((2*pi)^p * det_S)
    r_n = para$prob[k] * g_den
    r_nG[,k]= r_n
  }
  
  # Normalize
  Z = r_nG / rowSums(r_nG)  
  return(Z)
}

Mstep <- function(data, G, para, post.prob){ 

  # Return the updated parameters
  para.new <- list(prob = NULL, 
                   mean = NULL, 
                   Sigma = NULL, 
                   loglik = NULL)
  
  X = as.matrix(data)
  A = t(X)
  n = nrow(X)
  p = dim(X)[2]
  
  # new prob
  para.new$prob = apply(post.prob, 2, mean)  
  
  # new mu
  norm_w = colSums(post.prob)  
  para.new$mean = t(t(A %*% post.prob) / norm_w)
  
  # new Sigma
  c = matrix(0, p, p)
  for (k in 1:G) {
    # R works column by column, this is easier with python
    X_ = t(A - para.new$mean[,k])
    # my brain does not understand this diagonal trick
    c =  c + (t(X_) %*% diag(post.prob[,k]) %*% X_)
  }
  
  para.new$Sigma = c / n
  
  return(para.new)  
}

loglik <- function(data, G, para){
	# compute loglikelihood
  X = data
  A = t(X)
  p = dim(X)[2]
  inv_S = solve(para$Sigma)
  det_S = det(para$Sigma)
  loglikeli = 0
  
  for (i in 1:n) {
    logli_k = 0
    for (k in 1:G) {
      a = as.matrix(X[i, ] - para$mean[,k])
      b = exp(a %*% inv_S %*% t(a) / -2) / sqrt((2*pi)^p * det_S)
      logli_k = logli_k + (para$prob[k]  * b) 
    }
    loglikeli = loglikeli + log(logli_k)
  }
  
  return(loglikeli)
}

myEM <- function(data, itmax, G, para){
  # itmax: number of of iterations
  # G:     number of components
  # para:  list of (prob, mean, Sigma, loglik)
  
  for(t in 1:itmax){
    post.prob <- Estep(data, G, para)
    para <- Mstep(data, G, para, post.prob)
  }
  
  # update para$loglik
  para$loglik = loglik(data, G, para)
  
  return(para)
}
```



## Testing the functions


Functions are tested on the `faithful` data with $G = 2$ and $G = 3$ with **20** iterations (i.e., `itmax = 20`). The mixture model described above corresponds to `modelName = "EEE"` in `mclust`.

Set the number of printing digits to be eight. We expect the result from our function to agree with the result from `mclust`, up to the specified precision level. 

```{r}
options(digits=8)
```


### Load data

Load the `faithful` data from R package `mclust`.

```{r}
library(mclust)
```


### Comparing with two clusters

We **initialize** parameters by first randomly assigning the $n$ samples into two groups and then running one iteration of the built-in M-step. 

```{r}
n <- nrow(faithful)
G <- 2
uin_4 = 2110
set.seed(uin_4)
gID <- sample(1:G, n, replace = TRUE)
Z <- matrix(0, n, G)
for(k in 1:G)
  Z[gID == k, k] <- 1 
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
```

Here are the initial values we use for (prob, mean, Sigma).

```{r}
para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma, 
              loglik = NULL)

para0
```


* Output from `myEM` with `G=2` and `itmax=20`.  

```{r}
myEM(data=faithful, itmax=20, G=G, para=para0)
```

* Output from `mclust` with `G=2` and `itmax=20`.

```{r}
Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 20), 
           parameters = ini0)
list(prob = Rout$para$pro, mean = Rout$para$mean, 
     Sigma = Rout$para$variance$Sigma, 
     loglik = Rout$loglik)
```


### Comparing with three clusters

Similarly, we set $G=3$, then compare the result returned by `myEM` and the one returned by the em algorithm from `mclust` after 20 iterations.

```{r}
n <- nrow(faithful)
G <- 3
uin_4 = 2110
set.seed(uin_4)
gID <- sample(1:G, n, replace = TRUE)
Z <- matrix(0, n, G)
for(k in 1:G)
  Z[gID == k, k] <- 1 
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters
```

Here are the initial values we use for (prob, mean, Sigma).

```{r}
para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma, 
              loglik = NULL)

para0
```

* Output from `myEM` with `G=3` and `itmax=20`.

```{r}
myEM(data=faithful, itmax=20, G=G, para=para0)
```

* Output from `mclust` with `G=3` and `itmax=20`.

```{r}
Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 20), 
           parameters = ini0)
list(prob = Rout$para$pro, mean = Rout$para$mean, 
     Sigma = Rout$para$variance$Sigma, 
     loglik = Rout$loglik)
```

## Derivation

1. Expression of the marginal (or the so-called incomplete) likelihood function or its log, which is the objective function we aim to maximize.


$$
\begin{aligned}
& \prod_{i=1}^n  p(x_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \prod_{i=1}^n  \big[   p_1 N(x_i; \mu_1, \Sigma) + \cdots + p_G N(x_i; \mu_G, \Sigma) \big ]\\
= & \prod_{i=1}^n  \Big [ p_1  \frac{\exp  ( - \frac{1}{2} (x- \mu_1)^t \Sigma^{-1} (x - \mu_1)  )}{\sqrt{(2 \pi)^p | \Sigma| }}
 + \cdots + p_G \frac{\exp  ( - \frac{1}{2} (x- \mu_G)^t \Sigma^{-1} (x - \mu_G)  )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]
\end{aligned}
$$
where $|\Sigma|$ denotes the determinant of matrix $\Sigma$.

2. Expression of the complete likelihood function $\sum_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma)$ or its log, which is the function we work with in the EM algorithm.

$$
\begin{aligned}
& \prod_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \prod_{i=1}^n \prod_{k=1}^G   \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]^{\mathbb{1}_{\{Z_i = k \}}}
\end{aligned}
$$

3. Expression of the distribution of $Z_i$ at the E-step. 

    Given data and the current parameter value$(p_{1:G}^{(0)}, \mu_{1:G}^{(0)}, \Sigma^{(0)})$, $Z_i$ follows  a discrete distribute taking values from $1$ to $G$ with probabilities.
    
$$
\begin{aligned}
p_{ik} := & \mathbb{P}(Z_i = k \mid x_i, p_{1:G}^{(0)}, \mu_{1:G}^{(0)}, \Sigma^{(0)}) \\
= & \frac{p_k N(x_i; \mu_k, \Sigma_k)}{\sum_{k'=1}^G p_{k'} N(x_i; \mu_{k'}, \Sigma_{k'})} 
\end{aligned}
$$

4. Expression of the objective function we aim to maximize (or minimize) at the M-step.

At the M-step, we optimize the following objective function (where the expectation is taken over $Z_1, \dots, Z_n$ with respect to the probabilities computed at step 3): 
    
$$   
\begin{aligned}
g(p_{1:G}, \mu_{1:G}, \Sigma) = & \mathbb{E} \log \prod_{i=1}^n  p(x_i, Z_i \mid p_{1:G}, \mu_{1:G}, \Sigma) \\
= & \mathbb{E} \sum_{i=1}^n \sum_{k=1}^G  \mathbb{1}_{\{Z_i = k \}} \log  \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ] \\
= & \sum_{i=1}^n \sum_{k=1}^G  p_{ik}  \log  \Big [ p_k  \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ]
\end{aligned}
$$

where the last step is due to the fact that $\mathbb{E} [ \mathbb{1}_{\{Z_i = k \}}] = \mathbb{P}(Z_i = k) = p_{ik}.$

5. Derivation and the updating formulas for $p_{1:G}, \mu_{1:G}, \Sigma$ at the M-step.

We can rewrite the objective function as follows by re factoring and dropping some constants, namely $\sqrt{(2 \pi)^p}$


\begin{aligned}
\sum_{i=1}^n \sum_{k=1}^G p_{ik}\log p_k + \sum_{i=1}^n \sum_{k=1}^G  p_{ik}  \log  \Big [   \frac{\exp  ( - \frac{1}{2} (x- \mu_k)^t \Sigma^{-1} (x - \mu_k) )}{\sqrt{(2 \pi)^p | \Sigma| }} \Big ] & \\

\sum_{i=1}^n \sum_{k=1}^G p_{ik}\log p_k - \frac{1}{2} \sum_{i=1}^n \sum_{k=1}^G p_{ik} \log|\Sigma|  \Big [(x- \mu_k)^t \Sigma^{-1} (x - \mu_k) \Big ] &
\end{aligned}


For $\mu_k$, we take the partial derivative of the second term.


\begin{aligned}
 \frac{\partial }{\partial u_k} \Big  [ - \frac{1}{2} \sum_{i=1}^n \sum_{k=1}^G p_{ik}(x- \mu_k)^t \Sigma^{-1} (x - \mu_k) \Big ] \\

 - \frac{1}{2} \sum_{i=1}^n p_{ik} \frac{\partial }{\partial u_k} \Big [ \sum_{k=1}^G \mu_k^t \Sigma^{-1} \mu_k -2 x^t \Sigma^{-1} \mu_k\Big ] \\

 - \frac{1}{2} \sum_{i=1}^n p_{ik} \Big [2 \Sigma^{-1} \mu_k - 2 \Sigma^{-1} x \Big ] \\

 \sum_{i=1}^n p_{ik} \Big [\Sigma^{-1} x - \Sigma^{-1} \mu_k \Big ]

\end{aligned}


We set the equation to 0 to find the expression that optimizes $\mu_k$.

\begin{aligned}
\sum_{i=1}^n p_{ik} \Big [\Sigma^{-1} x - \Sigma^{-1} \mu_k \Big] = 0 \\

\sum_{i=1}^n p_{ik} \mu_k = \sum_{i=1}^n p_{ik} x \\

\mu_k = \frac{\sum_{i=1}^n p_{ik} x}{\sum_{i=1}^n p_{ik}} \\
\end{aligned}

For the derivative of $\Sigma_k$ we have:

\begin{aligned}
 \frac{\partial }{\partial \Sigma_k} \Big  [ - \frac{1}{2} \sum_{i=1}^n \sum_{k=1}^G p_{ik}  \log|\Sigma_k| (x- \mu_k)^t \Sigma_k^{-1} (x - \mu_k) \Big ] \\

 - \frac{1}{2} \sum_{i=1}^n p_{ik} \Big [\Sigma_k - (x- \mu_k)^t (x - \mu_k) \Big ]
\end{aligned}



\begin{aligned}
\sum_{i=1}^n p_{ik} \Big [\Sigma_k - (x- \mu_k)^t (x - \mu_k) \Big ] = 0 \\

\Sigma_k \sum_{i=1}^n p_{ik} = \sum_{i=1}^n p_{ik} (x- \mu_k)^t (x - \mu_k) \\

\Sigma_k = \frac{\sum_{i=1}^n p_{ik} (x- \mu_k)^t (x - \mu_k)}{\sum_{i'=1}^n p_{i'k}}
\end{aligned}


Lastly, for the derivation of the mixing proportions $p_k$ we introduced a `Langrange Multiplier` term.


\begin{aligned}
\frac{\partial }{\partial p_k} \Big [ \sum_{i=1}^n \sum_{k=1}^G p_{ik} \log p_k - \lambda(\sum_{k=1}^G p_k - 1)  \Big ] = & 0 \\

\frac{\sum_{i=1}^n p_{ik}}{p_k} - \lambda = & 0 \\

\sum_{i=1}^n p_{ik} = & \lambda {p_k} \\

\sum_{i=1}^n \sum_{k=1}^G p_{ik} = & \lambda \sum_{k=1}^G {p_k} \\

N = & \lambda \\

\end{aligned}


Once we know $\lambda$ for the `Langrange Multiplier` it follows that:


\begin{aligned}
& \frac{\sum_{i=1}^n p_{ik}}{p_k} - N = 0 \\

& p_k = \frac{1}{N} \sum_{i=1}^n p_{ik}
\end{aligned}


# PART II

## The Baum-Welch  algorihtm

The Baum-Welch Algorithm is the EM algorithm for HMM. We prepare a function `BW.onestep` to perform the E-step and M-step, and then iteratively call that function in `myBW`.

**We do not update w, since R's implementation does not do it.**

```{r}
myBW = function(x, para, n.iter = 100){
  # Input:
  # x: T-by-1 observation sequence
  # para: initial parameter value
  # Output updated para value (A and B; we do not update w)
  
  for(i in 1:n.iter){
    para = BW.onestep(x, para)
  }
  return(para)
}
```

`BW.onestep` function, in which we operate the E-step and M-step for one iteration. 

```{r}
BW.onestep = function(x, para){
  # Input: 
  # x: T-by-1 observation sequence
  # para: mx, mz, and current para values for
  #    A: initial estimate for mz-by-mz transition matrix
  #    B: initial estimate for mz-by-mx emission matrix
  #    w: initial estimate for mz-by-1 initial distribution over Z_1
  # Output the updated parameters after one iteration
  # We DO NOT update the initial distribution w
  
  T = length(x)
  mz = para$mz
  mx = para$mx
  A = para$A
  B = para$B
  w = para$w
  alp = forward.prob(x, para)
  beta = backward.prob(x, para)
  
  myGamma = array(0, dim=c(mz, mz, T-1))
  #######################################
  ## Compute gamma_t(i,j) P(Z[t] = i, Z[t+1]=j), 
  ## for t=1:T-1, i=1:mz, j=1:mz, 
  ## which are stored in an array, myGamma
  #######################################
  for(t in 1:(T-1)) {
    for(i in 1:mz) {
      for(j in 1:mz) {
        logs_ = log(c(alp[t, i], A[i, j], B[j, x[t+1]], beta[t+1, j]))
        myGamma[i, j, t] = exp(sum(logs_))
      }
    }
  }
  
  # M-step for parameter A
  #######################################
  ## A = ....
  #######################################
  newA = matrix(0, mz, mz)
  
  # sum all the mz by mz matrices
  for(t in 1:(T-1)) {
    newA = newA + myGamma[,,t]
  }
  # Convert to probability vectors for each Zi
  newA = newA / rowSums(newA)
  
  # M-step for parameter B
  #######################################
  ## B = ....
  #######################################
  
  newB = matrix(0, mz, mx)

  for (i in 1:mz) {
    for (l in 1:mx) {
      Ts = which(x == l)
      if (any(Ts==T)) {
        newB[i,l] = sum(myGamma[,i,T-1]) 
      }
      Ts = Ts[Ts != T]
      newB[i,l] = newB[i,l]+sum(myGamma[i,,Ts]) 
    }
  }
  
  newB = newB / rowSums(newB)
  
  para$A = newA
  para$B = newB
  return(para)
}
```

The following functions compute the forward and backward probabilities.

```{r}
forward.prob = function(x, para){
  # Output the forward probability matrix alp 
  # alp: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  alp = matrix(0, T, mz)
  
  # fill in the first row of alp
  alp[1, ] = w * B[, x[1]]
  # Recursively compute the remaining rows of alp
  for(t in 2:T){
    tmp = alp[t-1, ] %*% A
    alp[t, ] = tmp * B[, x[t]]
    }
  return(alp)
}

backward.prob = function(x, para){
  # Output the backward probability matrix beta
  # beta: T by mz, (t, i) entry = P(x_{1:t}, Z_t = i)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  beta = matrix(1, T, mz)

  # The last row of beta is all 1.
  # Recursively compute the previous rows of beta
  for(t in (T-1):1){
    tmp = as.matrix(beta[t+1, ] * B[, x[t+1]])  # make tmp a column vector
    beta[t, ] = t(A %*% tmp)
    }
  return(beta)
}
```

## The Viterbi algorihtm

The Viterbi algorithm returns the most probable latent sequence given the data and the MLE of parameters. 

```{r}
myViterbi = function(x, para){
  # Output: most likely sequence of Z (T-by-1)
  T = length(x)
  mz = para$mz
  A = para$A
  B = para$B
  w = para$w
  log.A = log(A)
  log.w = log(w)
  log.B = log(B)
  
  # Compute delta (in log-scale)
  delta = matrix(0, T, mz) 
  # fill in the first row of delta
  delta[1, ] = log.w + log.B[, x[1]]
  
  #######################################
  ## Recursively compute the remaining rows of delta
  #######################################

  for (t in 2:T) {
    for (i in 1:mz) {
      delta[t, i] = log.B[i, x[t]] + max(delta[t - 1, ] + log.A[, i])
    }
  }
  
  # Compute the most prob sequence Z
  Z = rep(0, T)
  # start with the last entry of Z
  Z[T] = which.max(delta[T, ])
  
  #######################################
  ## Recursively compute the remaining entries of Z
  #######################################

  for (t in (T - 1):1) {
    Z[t] = which.max(delta[t, ] + log.A[, Z[t + 1]] )
  }
  
  return(Z)
}
```

## Testing the functions

### Baum-Welch results

Below we initialize the parameters and use the data in `coding4_part2_data.txt` to compare our implementation with R's after `100` iterations.

```{r}
data = scan("coding4_part2_data.txt")

mz = 2
mx = 3
ini.w = rep(1, mz)
ini.w = ini.w / sum(ini.w)
ini.A = matrix(1, 2, 2)
ini.A = ini.A / rowSums(ini.A)
ini.B = matrix(1:6, 2, 3)
ini.B = ini.B / rowSums(ini.B)
ini.para = list(mz = 2, mx = 3, w = ini.w,
                A = ini.A, B = ini.B)

myout = myBW(data, ini.para, n.iter = 100)


# Calling library HMM
library(HMM)
hmm0 =initHMM(c("A", "B"), c(1, 2, 3),
              startProbs = ini.w,
              transProbs = ini.A, 
              emissionProbs = ini.B)
Rout = baumWelch(hmm0, data, maxIterations=100, delta=1E-9, pseudoCount=0)

```

```{r}
options(digits=8)
```

- Compare estimates for transition prob matrix A.

```{r}
myout$A
Rout$hmm$transProbs
```

- Compare estimates for emission prob matrix B.

```{r}
myout$B
Rout$hmm$emissionProbs
```

### Viterbi results

Again we compare our function `myViterbi` with R's implementation. Note that for our function we provide the `A` and `B` matrices produced by R's `baumWelch` function. This is as per the assignment instructions.

```{r}

# Using output  from library as per instructions to reduce risk of double penalization.
para = list(mz = mz, mx = mx, w = ini.w,
            A = Rout$hmm$transProbs, B = Rout$hmm$emissionProbs)

myout.Z = myViterbi(data, para)
myout.Z[myout.Z==1] = 'A'
myout.Z[myout.Z==2] = 'B'

# Calling library
Rout.Z = viterbi(Rout$hmm, data)
```

- Compare the most probable Z sequence.

```{r}
cbind(Rout.Z, myout.Z)[c(1:10, 180:200), ]
sum(Rout.Z != myout.Z)
```

