---
title: "Coding 2 - PSL 598, Fall 2022"
author: "Alejandro Pimentel (netID ap41,UIN 659282110)"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
    toc_float: TRUE
    code_folding: show
    code_download: true
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{r message=FALSE, warning=FALSE}
library(glmnet) 
library(pls)
```

## Part I: Implement Lasso

### One-variable Lasso

The `one_var_lasso` function below takes the following inputs:

$$\mathbf{r}=(r_1, \dots, r_n)^t, \quad \mathbf{z} = (z_1, \dots, z_n)^t, \quad  \lambda > 0$$
and solves the following one-variable Lasso problem:
$$ 
  \min_{b} \frac{1}{2n}  \sum_{i=1}^n (r_i- b z_{i} )^2 + \lambda | b | = \min_{b} \frac{1}{2n} \| \mathbf{r} - b \cdot \mathbf{z}\|^2 + \lambda  | b |.
$$

```{r}
one_var_lasso = function(r, z, lam) {
 
  n = length(r)
  
  z_len = t(z) %*% z
  a_num = t(r) %*% z
  
  a = a_num / z_len
  lam_ = (2*n*lam) / z_len
  
  ## derivation cases
  x = a + lam_ / 2
  if(a > lam_ / 2) {
    x = a - lam_ / 2
  } 
  
  if(abs(a) <= lam_ / 2) {
    x = 0
  }
  
  ## select same sign as a
  if((a < 0 && x > 0) || (a > 0 && x < 0)) {
    x = x * -1
  } 
  
  return(x)
}
```

### The CD Algorithm

Function `MyLasso` implements the **Coordinate Descent (CD)** algorithm by repeatedly calling `one_var_lasso`. 

In the CD algorithm, at each iteration, we solve a one-variable Lasso problem for $\beta_j$ while holding the other $(p-1)$ coefficients at their current values:
$$ 
\min_{\beta_j} \sum_{i=1}^n (\color{blue}{y_i - \sum_{k \ne j} x_{ik} \beta_k} - x_{ij} \beta_j)^2 + \lambda \sum_{k \ne j} | \beta_k| + \lambda | \beta_j|,
$$
which is equivalent to solving the following one-variable Lasso problem
$$
\min_{\beta_j} \sum_{i=1}^n (\color{blue}{r_i}- x_{ij} \beta_j)^2 + \lambda | \beta_j|, \quad \color{blue}{r_i}  = y_i - \sum_{k \ne j} x_{ik}  \beta_k.
$$

```{r}
MyLasso = function(X, y, lam.seq, maxit = 100) {
    
    # Input
    # X: n-by-p design matrix without the intercept 
    # y: n-by-1 response vector 
    # lam.seq: sequence of lambda values (arranged from large to small)
    # maxit: number of updates for each lambda 
    
    # Output
    # B: a (p+1)-by-length(lam.seq) coefficient matrix 
    #    with the first row being the intercept sequence

    n = length(y)
    p = dim(X)[2]
    nlam = length(lam.seq)
    B = matrix(0, ncol = nlam, nrow = (p+1))
    rownames(B) = c("Intercept", colnames(X)) 
    
    # new.X = centered & scaled X;
    ones = rep(1, n)
    X_means = colMeans(X)
    Xn_means = ones %*% t(X_means)
    X_centered = X - Xn_means
    
    X_sds = apply(X, 2, sd)
    Xn_sds = ones %*% t(X_sds)
    new.X = X_centered / Xn_sds    

    # Initialize coef vector b and residual vector r
    b = rep(0, p)
    r = y
    
    # Triple nested loop
    for (m in 1:nlam) {
      for (step in 1:maxit) {
        for (j in 1:p) {
          r = r + (new.X[, j] * b[j])
          b[j] = one_var_lasso(r, new.X[, j], lam.seq[m])
          r = r - new.X[, j] * b[j]
        }
      }
      B[-1, m] = b
    }

    # scale back the coefficients;
    # update the intercepts stored in B[1, ]
    B[-1, ] = B[-1, ] / X_sds
    inter_=colSums(B[-1,] * -X_means)
    B[1,] = mean(y) + inter_
    
    return(B)
}
```

### Test the Lasso Function

We test `MyLasso` function on data set `Coding2_Data.csv` with the following lambda sequence. Data set has 13 predictors, `V1` to `V13`, and one response vector `Y`.

```{r}
myData = read.csv("Coding2_Data.csv")
X = as.matrix(myData[, -14])
y = myData$Y
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq)
```


* Next we check the accuracy of the function against the output from `glmnet`. The maximum difference between the two coefficient matrices should be <span style="color: red;">less than 0.005</span>.

```{r}

lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
max(abs(coef(lasso.fit) - myout))
```

* Below we show a path plot for the 13 non-intercept coefficients obtained with `MyLasso` function along the lambda values in log scale.


```{r}
x.index = log(lam.seq)
beta = myout[-1, ]  # beta is a 13-by-80 matrix
matplot(x.index, t(beta),
        xlim = c(min(x.index), max(x.index)),
        lty = 1,
        xlab = "Log Lambda",
        ylab = "Coefficients",
        type="l", 
        lwd = 1)
# You can add variable names to each path
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)
```


Below, the plot from `glmnet` fit is very similar to the one using `MyLasso`.

```{r}
plot(lasso.fit, xvar = "lambda")
```



## Part II: Simulation Study


```{r}
# Set a seed to have a stable generation of random numbers
uin_4 = 2110
set.seed(uin_4)
```

First, we define functions to repeat **seven** different procedures multiple times in a loop.

* __Full__: linear regression model using all features

```{r}
full_model = function(data, test.ids) {

  mse = rep(NaN, dim(test.ids)[2])
  for(i in 1:dim(test.ids)[2]) {
    test.id = test.ids[, i] 
    full.model = lm(Y ~ ., data = data[-test.id, ])
    Ytest.pred = predict(full.model, newdata = myData[test.id, ])
    mse[i] = mean((myData$Y[test.id] - Ytest.pred)^2)
  }
  
  return(mse)
}
```


* __Ridge.min__ and __Ridge.1se__: Ridge regression using `lambda.min` or `lambda.1se` 

```{r}
ridge_reg = function(X_data, Y_data, test.ids) {
  
  mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
  
  mse = list(
    lam_min = rep(NaN, dim(test.ids)[2]),
    lam_1se = rep(NaN, dim(test.ids)[2])
  )
  
  for(i in 1:dim(test.ids)[2]) {  
    test.id = test.ids[, i]
    cv.out = cv.glmnet(X_data[-test.id, ], Y_data[-test.id], alpha = 0, 
                       lambda = mylasso.lambda.seq)
    
    best.lam = cv.out$lambda.min
    Ytest.pred = predict(cv.out, s = best.lam, newx = X_data[test.id, ])
    mse$lam_min[i] = mean((Y_data[test.id] - Ytest.pred)^2)
    
    best.lam = cv.out$lambda.1se
    Ytest.pred = predict(cv.out, s = best.lam, newx = X_data[test.id, ])
    mse$lam_1se[i] = mean((Y_data[test.id] - Ytest.pred)^2)
  }
  
  return(mse)
}
```


* __Lasso.min__ and __Lasso.1se__: Lasso using `lambda.min` or `lambda.1se`
* __L.Refit__: Refit the model selected by Lasso using `lambda.1se`

```{r}
lasso_reg = function(data, test.ids) {
  
  X_data = data.matrix(data[,-1])  
  Y_data = data[,1] 
  
  mse = list(
    lam_min = rep(NaN, dim(test.ids)[2]),
    lam_1se = rep(NaN, dim(test.ids)[2]),
    refit = rep(NaN, dim(test.ids)[2])
  )
  
  for(i in 1:dim(test.ids)[2]) {  
    test.id = test.ids[, i]
    cv.out = cv.glmnet(X_data[-test.id, ], Y_data[-test.id], alpha = 1)
    best.lam = cv.out$lambda.min
    Ytest.pred = predict(cv.out, s = best.lam, newx = X_data[test.id, ])
    mse$lam_min[i] = mean((Y_data[test.id] - Ytest.pred)^2)
    
    best.lam = cv.out$lambda.1se
    Ytest.pred = predict(cv.out, s = best.lam, newx = X_data[test.id, ])
    mse$lam_1se[i] = mean((Y_data[test.id] - Ytest.pred)^2)
    
    # Lasso refit
    mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
    var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]

    mylasso.refit = lm(Y ~ ., data[-test.id, c("Y", var.sel)])
    Ytest.pred = predict(mylasso.refit, newdata = data[test.id, ])
    mse$refit[i] = mean((Ytest.pred - Y_data[test.id])^2)
  }
  
  return(mse)
}

```


* __PCR__: principle components regression with the number of components chosen by 10-fold cross validation

```{r}
pcr_reg = function(data, test.ids) {
  
  mse = rep(NaN, dim(test.ids)[2])
  
  for(i in 1:dim(test.ids)[2]) {
    test.id = test.ids[, i]
    mypcr = pcr(Y ~ ., data= data[-test.id, ], validation="CV", scale=TRUE)
    CVerr = RMSEP(mypcr)$val[1, , ]
    adjCVerr = RMSEP(mypcr)$val[2, , ]
    best.ncomp = which.min(CVerr) - 1 
    
    if (best.ncomp==0) {
      Ytest.pred = mean(data$Y[-test.id])
    } else {
      Ytest.pred = predict(mypcr, data[test.id,], ncomp=best.ncomp)
    }
    mse[i] = mean((Ytest.pred - data$Y[test.id])^2)
  }
  
  return(mse)
}
```


### Case I

For __Case I__ we use the data set `Coding2_Data2.csv`. The first 14 columns are the same as the data set we used in Part I with `Y` being the response variable (moved to the 1st column). The additional 78 more predictors are the quadratic and interaction terms of the original 13 predictors.

```{r}
myData = read.csv("Coding2_Data2.csv", header = TRUE)

X = data.matrix(myData[,-1])  
Y = myData[,1] 

# We repeat each of the 7 procedures T times
T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 

# sample test ids T times
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
```


* [a] Below we run all functions defined and collect the seven `MSPE`.

```{r}
full_c1 = full_model(myData, all.test.id)
ridge_c1 = ridge_reg(X, Y, all.test.id)
lasso_c1 = lasso_reg(myData, all.test.id)
pcr_c1 = pcr_reg(myData, all.test.id)

df_results_c1 = data.frame(
  "Full" = full_c1,
  "Ridge.min" = ridge_c1$lam_min,
  "Ridge.1se" = ridge_c1$lam_1se,
  "Lasso.min" = lasso_c1$lam_min,
  "Lasso.1se" = lasso_c1$lam_1se,
  "L.refit" = lasso_c1$refit,
  "PCR" = pcr_c1
)
```


* [b] Next we compare results on MSPE graphically, using a boxplot.

```{r fig1, fig.width = 8, fig.height=6}

boxplot(df_results_c1,  col = terrain.colors(dim(df_results_c1)[2], alpha = 0.5))

for(i in 1:dim(df_results_c1)[2]) {
  mses = df_results_c1[, i]
  myjitter = jitter(rep(i, length(mses)), amount = 0.2)
  points(myjitter, mses, pch=20, col=rgb(0,0,0,.9))
}
```

Summarize `MSPE` results.

```{r}
summary(df_results_c1)
```

Obtain the mean `MSPE` for each procedure.

```{r}
apply(df_results_c1, 2, sd)
```


* [c] Based on your simulation results, answer the following questions: 
  * Which method or methods perform the best? 
    * `Ridge.min`, `Lasso.min` and `PCR` seem to have the lowest MSPE on average.
  
  * Which method or methods perform the worst? 
    * `Full`, `Ridge.1se`, `Lasso.1se` and `L.refit` seem to perform worst.
  
  * For Ridge regression, which, `lambda.min` or `lambda.1se`, produces better MSPE? 
    * `Ridge` with `lambda.min` produces lower MSPE.
  
  * For Lasso regression, which, `lambda.min` or `lambda.1se`, produces better MSPE? 
    * `Lasso` with `lambda.min` produces lower MSPE.
  
  
  * Is it worth doing refit? That is, does __L.Refit__ perform better than __Lasso.1se__? 
    * `L.Refit` does not perform significantly better than `Lasso.1se`.
  
  * Is it worth doing variable selection or shrinkage in this case? That is, do you think the performance of __Full__ is comparable or not to the best among the other six?  
    * `Full` model fit seems to perform worst than some of the selection or shrinkage methods, so we can conclude it is worth it.


### Case II

For __Case II__ we use the data set `Coding2_Data3.csv`. The first 92 columns are the same as `Coding2_Data2.csv`, and the remaining 500 columns are artificially generated noise features.

```{r}
myData = read.csv("Coding2_Data3.csv", header = TRUE)

X = data.matrix(myData[,-1])  
Y = myData[,1] 

T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
```

* Execute procedures (except for `Full`) and show results.

```{r}
ridge_c2 = ridge_reg(X, Y, all.test.id)
lasso_c2 = lasso_reg(myData, all.test.id)
pcr_c2 = pcr_reg(myData, all.test.id)

df_results_c2 = data.frame(
  "Ridge.min" = ridge_c2$lam_min,
  "Ridge.1se" = ridge_c2$lam_1se,
  "Lasso.min" = lasso_c2$lam_min,
  "Lasso.1se" = lasso_c2$lam_1se,
  "L.refit" = lasso_c2$refit,
  "PCR" = pcr_c2
)
```

```{r fig2, fig.width = 8, fig.height=6}
boxplot(df_results_c2,  col = terrain.colors(dim(df_results_c2)[2], alpha = 0.5))

for(i in 1:dim(df_results_c2)[2]) {
  mses = df_results_c2[, i]
  myjitter = jitter(rep(i, length(mses)), amount = 0.2)
  points(myjitter, mses, pch=20, col=rgb(0,0,0,.9))
}
```

Summarize `MSPE` results.

```{r}
summary(df_results_c2)
```

Obtain the mean `MSPE` for each procedure.

```{r}
apply(df_results_c2, 2, sd)
```

* [c] Answer the following questions: 
  * Which method or methods perform the best in this case? 
    * `Lasso.min`, `Lasso.1se` and `L.refit` seem to perform better in this case.
  
  * Which method or methods perform the worst in this case?
    * `PCR` has the highest MSPE, followed by `Ridge.min` and `Ridge.1se`.
    
  * Do you notice any method/methods that performs/perform better in one case but not in the other? If so, explain why.
    * On __Case I__ `Ridge.min`, `Ridge.1se` and `PCR` seemed to do better than on __Case II__.
    * Even more, all methods perform better on __Case I__.
    * Noise introduced on __Case II__ seems to be playing as a factor to the overall drop in performance. We try to explain this in the next point.
  
  * Since `Coding2_Data3.csv` contains all features in `Coding2_Data2.csv`, one may expect that the best MSPE in Case II is smaller than, or at least not bigger than, the best MSPE in Case I. Do your simulation results support this expectation? If not, explain why. 
    * The simulation study indicates that even though `Coding2_Data3.csv` contains all features in `Coding2_Data2.csv`, all `MSPE` are better in average on `Coding2_Data2.csv`, __Case I__. So the aforementioned expectation is not supported.
    * On `Coding2_Data3.csv` we introduced 500 predictors that are random noise, methods like Ridge (which do shrinkage only) will try to reduce the influence of the noisy predictors by shrinking the corresponding coefficients $\hat\beta_j$ close to `0`. But not all that influence is eliminated.
    * Similarly PCR gets affected while finding the direction of the principal components in the presence of the noisy predictors.
    * Note that among the methods used for __Case II__, `Lasso` performs better since it also does selection, in other words, likely many of the coefficients for the noisy predictors are set to `0`.
  

