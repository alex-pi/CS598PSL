---
title: "(PSL) Coding Assignment 2"
date: "Fall 2022"
output:
  html_notebook:
    theme: readable
    toc: TRUE
    toc_float: TRUE
---

## Part I: Implement Lasso

### One-variable Lasso

First write a function `one_var_lasso` that takes the following inputs:

$$\mathbf{r}=(r_1, \dots, r_n)^t, \quad \mathbf{z} = (z_1, \dots, z_n)^t, \quad  \lambda > 0$$
and solves the following one-variable Lasso problem:
$$ 
  \min_{b} \frac{1}{2n}  \sum_{i=1}^n (r_i- b z_{i} )^2 + \lambda | b | = \min_{b} \frac{1}{2n} \| \mathbf{r} - b \cdot \mathbf{z}\|^2 + \lambda  | b |.
$$
Check the [[derivation](https://liangfgithub.github.io/F22/OneVarLasso.pdf)] for one-variable lasso.

```{r}
one_var_lasso = function(r, z, lam) {
 
  ##############################
  # YOUR CODE
  ##############################
}
```

### The CD Algorithm

Next write your own function `MyLasso` to implement the **Coordinate Descent (CD)** algorithm by repeatedly calling `one_var_lasso`. 

In the CD algorithm, at each iteration, we solve a one-variable Lasso problem for $\beta_j$ while holding the other $(p-1)$ coefficients at their current values:
$$ 
\min_{\beta_j} \sum_{i=1}^n (\color{blue}{y_i - \sum_{k \ne j} x_{ik} \beta_k} - x_{ij} \beta_j)^2 + \lambda \sum_{k \ne j} | \beta_k| + \lambda | \beta_j|,
$$
which is equivalent to solving the following one-variable Lasso problem
$$
\min_{\beta_j} \sum_{i=1}^n (\color{blue}{r_i}- x_{ij} \beta_j)^2 + \lambda | \beta_j|, \quad \color{blue}{r_i}  = y_i - \sum_{k \ne j} x_{ik}  \beta_k.
$$

Your function should output estimated Lasso coefficients similar to the ones returned by R with option __`standardized = TRUE`__. 

Your function may look like the following. 
```{r}
MyLasso = function(X, y, lam.seq, maxit = 100) {
    
    # Input
    # X: n-by-p design matrix without the intercept 
    # y: n-by-1 response vector 
    # lam.seq: sequence of lambda values (arranged from large to small)
    # maxit: number of updates for each lambda 
    
    # Output
    # B: a (p+1)-by-length(lam.seq) coefficient matrix 
    #    with the first row being the intercept sequence

    n = length(y)
    p = dim(X)[2]
    nlam = length(lam.seq)
    B = matrix(0, ncol = nlam, nrow = (p+1))
    rownames(B) = c("Intercept", colnames(X)) 

    ##############################
    # YOUR CODE: 
    # (1) new.X = centered & scaled X; 
    # (2) record the centers and scales used in (1) 
    ##############################

    # Initialize coef vector b and residual vector r
    b = rep(0, p)
    r = y
    
    # Triple nested loop
    for (m in 1:nlam) {
      for (step in 1:maxit) {
        for (j in 1:p) {
          r = r + (new.X[, j] * b[j])
          b[j] = one_var_lasso(r, new.X[, j], lam.seq[m])
          r = r - new.X[, j] * b[j]
        }
      }
      B[-1, m] = b
    }
   
    ##############################
    # YOUR CODE:
    # scale back the coefficients;
    # update the intercepts stored in B[1, ]
    ##############################
    
    return(B)
}
```

### Test Your Function

Test your `MyLasso` function on data set `Coding2_Data.csv` with the following lambda sequence. The data set `Coding2_Data.csv` can be downloaded [[Here](https://liangfgithub.github.io/F22/Coding2_Data.csv)]; it has 13 predictors, `V1` to `V13`, and one response vector `Y`.

```{r}
myData = read.csv("Coding2_Data.csv")
X = as.matrix(myData[, -14])
y = myData$Y
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq)
```


* Check the accuracy of your function against the output from `glmnet`. The maximum difference between the two coefficient matrices should be <span style="color: red;">less than 0.005</span>.

```{r}
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
max(abs(coef(lasso.fit) - myout))
```

* Produce a path plot for the 13 non-intercept coefficients along the lambda values in log scale.


```{r}
x.index = log(lam.seq)
beta = myout[-1, ]  # beta is a 13-by-80 matrix
matplot(x.index, t(beta),
        xlim = c(min(x.index), max(x.index)),
        lty = 1,
        xlab = "Log Lambda",
        ylab = "Coefficients",
        type="l", 
        lwd = 1)
# You can add variable names to each path
var.names = colnames(X)
nvar = length(var.names)
xpos = rep(min(x.index), nvar)
ypos = beta[, ncol(beta)]
text(xpos, ypos, var.names, cex=0.5, pos=2)
```
![](coding2_mylasso_plot.png)

Your plot should look almost the same as the plot from `glmnet`
```{r}
plot(lasso.fit, xvar = "lambda")
```
![](coding2_glmnet_plot.png)

## Part II: Simulation Study

Consider the following **seven** procedures:

* __Full__: run a linear regression model using all features
* __Ridge.min__ and __Ridge.1se__: Ridge regression using `lambda.min` or `lambda.1se` 
* __Lasso.min__ and __Lasso.1se__: Lasso using `lambda.min` or `lambda.1se`
* __L.Refit__: Refit the model selected by Lasso using `lambda.1se`
* __PCR__: principle components regression with the number of components chosen by 10-fold cross validation

### Case I

Download `Coding2_Data2.csv` [[Link](https://liangfgithub.github.io/F22/Coding2_Data2.csv)]. The first 14 columns are the same as the data set we used in Part I with `Y` being the response variable (moved to the 1st column). The additional 78 more predictors are the quadratic and interaction terms of the original 13 predictors.

* [a] Repeat the following simulation 50 times: In each iteration, randomly split the data into two parts, 75% for training and 25% for testing. For each of the **seven** procedures, fit a model based on the training data and obtain a prediction on the test data, record the mean squared prediction error (MSPE) on the test data.

* [b] Summarize your results on MSPE graphically, e.g., using boxplot or stripchart. 

* [c] Based on your simulation results, answer the following questions: 
  * Which method or methods perform the best? 
  * Which method or methods perform the worst? 
  * For Ridge regression, which, `lambda.min` or `lambda.1se`, produces better MSPE? 
  * For Lasso regression, which, `lambda.min` or `lambda.1se`, produces better MSPE? 
  * Is it worth doing refit? That is, does __L.Refit__ perform better than __Lasso.1se__? 
  * Is it worth doing variable selection or shrinkage in this case? That is, do you think the performance of __Full__ is comparable or not to the best among the other six?  


### Case II

Download `Coding2_Data3.csv` [[Link](https://liangfgithub.github.io/F22/Coding2_Data3.csv)]. The first 92 columns are the same as `Coding2_Data2.csv`, and the remaining 500 columns are artificially generated noise features.

* Repeat [a] and [b] above for the seven procedures **except Full**, and summarize your results on MSPE graphically. 

* [c] Answer the following questions: 
  * Which method or methods perform the best in this case? 
  * Which method or methods perform the worst in this case?
  * Do you notice any method/methods that performs/perform better in one case but not in the other? If so, explain why. 
  * Since `Coding2_Data3.csv` contains all features in `Coding2_Data2.csv`, one may expect that the best MSPE in Case II is smaller than, or at least not bigger than, the the best MSPE in Case I. Do your simulation results support this expectation? If not, explain why. 
  
### Sample Code

Load libraries and data. 

```{r}
library(glmnet) 
library(pls)
myData = read.csv("Coding2_Data2.csv", header = TRUE)
```  

Some algorithms need the matrix/vector input (instead of a data frame)
```{r}
X = data.matrix(myData[,-1])  
Y = myData[,1] 
```

We will repeat the simulation 50 times. In each iteration, randomly split the data into two parts, 75% for training and 25% for testing. You can write a loop with 50 iterations, and in each iteration, split the data and run the seven procedures (six procedures for case II).

Or you can save the row IDs for the 50 test data sets, then write a separate loop for each method. Using `all.test.id` (produced below), you can ensure that you use the same training/test split for each procedure.

```{r}
T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
#save(all.test.id, file="alltestID.RData")
```

Next, let's try the seven procedures using the first split. 

* __Full Model__
```{r}
test.id = all.test.id[, 1] 
full.model = lm(Y ~ ., data = myData[-test.id, ])
Ytest.pred = predict(full.model, newdata = myData[test.id, ])
mean((myData$Y[test.id] - Ytest.pred)^2)
```

* __Ridge Regression__

The default lambda sequence for this data set is too large. Suggest to use a sequence that covers smaller lambda values as `mylasso.lambda.seq` below.

```{r}
mylasso.lambda.seq = exp(seq(-10, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                   lambda = mylasso.lambda.seq)

best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
mean((Y[test.id] - Ytest.pred)^2)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
mean((Y[test.id] - Ytest.pred)^2)
```

* __Lasso__ 
```{r}
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
mean((Y[test.id] - Ytest.pred)^2)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
mean((Y[test.id] - Ytest.pred)^2)

# Lasso refit
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
mean((Ytest.pred - Y[test.id])^2)
```

* __PCR__

The principle components regression command, `pcr`, returns both the CV errors and the adjusted CV errors. For the definition of adjusted CV used in `pcr`, check Sec 2.4 of [this paper](https://mevik.net/work/publications/MSEP_estimates.pdf); we use CV error below. 

```{r}
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1 

if (best.ncomp==0) {
    Ytest.pred = mean(myData$Y[-test.id])
  } else {
    Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
  }
mean((Ytest.pred - myData$Y[test.id])^2)
```

Note that we have to subtract one from `which.min(CVerr)` since the 1st column of the CV table corresponds to the CV error with zero component (i.e., the model with just the intercept) and the k-th column of the CV table corresponds to (k-1) components.

The prediction function does not seem to work when `best.ncomp = 0`. So we have to handle that case separately.

---

## What to Submit
* A Markdown (or Notebook) file in HTML format, which contains all necessary code and the corresponding output/results.

* Set the seed at the beginning of Part II to be the last 4-dig of your UIN. So we can get the same simulation results if we re-run your code, 

* Name your file starting with 
  
  <span style="color: red;">Assignment_2_xxxx netID</span>
  
  where “xxxx” is the last 4-dig of your UIN and make sure the same 4-dig is used as the seed in your code.
  
